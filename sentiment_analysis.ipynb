{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting sentiment with ML (+ 80% accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# XGBoost for Machine Learning (Gradient Boosting Machine (GBM))\n",
    "import xgboost as xgb\n",
    "\n",
    "# nltk for preprocessing of text data\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Random seeds for consistent results\n",
    "#from tensorflow import set_random_seed\n",
    "seed = 1234\n",
    "#np.random.seed(seed)\n",
    "#set_random_seed(seed)\n",
    "\n",
    "# sklearn for preprocessing and machine learning models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics  import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Keras for neural networks\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Tweets.csv',sep=',', delimiter=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14640, 15)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.ndim) #2\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence',\n",
       "       'negativereason', 'negativereason_confidence', 'airline',\n",
       "       'airline_sentiment_gold', 'name', 'negativereason_gold',\n",
       "       'retweet_count', 'text', 'tweet_coord', 'tweet_created',\n",
       "       'tweet_location', 'user_timezone'], dtype=object)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selected DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                               text  \\\n",
       "0  570306133677760513                @VirginAmerica What @dhepburn said.   \n",
       "1  570301130888122368  @VirginAmerica plus you've added commercials t...   \n",
       "\n",
       "  airline_sentiment  \n",
       "0           neutral  \n",
       "1          positive  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select features\n",
    "df = df[['tweet_id', 'text', 'airline_sentiment']]\n",
    "print('Feature selected DataFrame:')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot sentiment distribution\n",
    "df['airline_sentiment'].value_counts().plot(kind = 'barh',\n",
    "                                            figsize = (15,10));\n",
    "plt.title('Distribution of airline sentiment in Kaggle dataset', \n",
    "          fontsize = 26, weight = 'bold')\n",
    "plt.xlabel('Frequency', fontsize = 20)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.yticks(fontsize = 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['God', 'is', 'Great', '!', 'I', 'won', 'a', 'lottery', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"God is Great! I won a lottery.\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "class PreProcessor:\n",
    "    def __init__(self, df, column_name):\n",
    "        self.data = df\n",
    "        self.conversations = list(self.data[column_name])\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.stemmer = SnowballStemmer(\"english\")\n",
    "        self.preprocessed = []\n",
    "    \n",
    "    # Tokenization\n",
    "    def tokenize(self, sentence):\n",
    "        '''\n",
    "        Splits up words and makes a list of all words in the tweet\n",
    "        '''\n",
    "        tokenized_sentence = word_tokenize(sentence)\n",
    "        return tokenized_sentence\n",
    "    \n",
    "    # Removing stop words\n",
    "    def remove_stopwords(self, sentence):\n",
    "        '''Removes stopwords like 'a', 'the', 'and', etc.'''\n",
    "        stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "        filtered_sentence = []\n",
    "        for w in sentence:\n",
    "            if w not in stop_words and len(w) > 1 and w[:2] != '//' and w != 'https': \n",
    "                filtered_sentence.append(w)\n",
    "        return filtered_sentence\n",
    "    \n",
    "    # Stemming\n",
    "    def stem(self, sentence):\n",
    "        '''\n",
    "        Stems certain words to their root form.\n",
    "        For example, words like 'computer', 'computation'\n",
    "        all get truncated to 'comput'\n",
    "        '''\n",
    "        return [self.stemmer.stem(word) for word in sentence]\n",
    "    \n",
    "    # Transform the tokens back to one string\n",
    "    def join_to_string(self, sentence):\n",
    "        '''\n",
    "        Joins the tokenized words to one string.\n",
    "        '''\n",
    "        return ' '.join(sentence)\n",
    "    \n",
    "    def full_preprocess(self, n_rows=None):\n",
    "        '''\n",
    "        Preprocess a selected number of rows and\n",
    "        connects them back to strings\n",
    "        '''\n",
    "        # If nothing is given do it for the whole dataset\n",
    "        if n_rows == None:\n",
    "            n_rows = len(self.data)\n",
    "            \n",
    "        # Perform preprocessing\n",
    "        for i in range(n_rows):\n",
    "            tweet = self.conversations[i]\n",
    "            tokenized = self.tokenize(tweet)\n",
    "            cleaned = self.remove_stopwords(tokenized)\n",
    "            stemmed = self.stem(cleaned)\n",
    "            joined = self.join_to_string(stemmed)\n",
    "            self.preprocessed.append(joined)\n",
    "        return self.preprocessed\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text and put it in a new column\n",
    "preprocessor = PreProcessor(df, 'text')\n",
    "df['cleaned_text'] = preprocessor.full_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Split data into train, validation and test sets\n",
    "\n",
    "We split the data into a training set, validation set and test set. This is crucial for training and evaluation of good machine learning models.\n",
    "\n",
    "The data will be shuffled and we take 1000 tweets for our test set. The remaining data will be split 80/20. This means that 80% will be used for training and 20% for validation set. During the model training phase our goal is to maximize the accuracy on the validation set. The test set is to check that our model truly generalizes to data it has never seen before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling so we can get random tweets for the test set\n",
    "df = df.sample(frac = 1) \n",
    "\n",
    "# Keep 1000 samples of the data as test set\n",
    "test_set = df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>569988078015815680</td>\n",
       "      <td>@United will not have to honor absurdly low mi...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>unit honor absurd low mistak fare http vía usa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>567793696956231680</td>\n",
       "      <td>@united thanks for the offer, but I finally ma...</td>\n",
       "      <td>negative</td>\n",
       "      <td>unit thank offer final made destin albeit hour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>567846970141716481</td>\n",
       "      <td>@VirginAmerica @JezzieGoldz would have been a ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>virginamerica jezziegoldz would rough trip luc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12686</th>\n",
       "      <td>570062369096908801</td>\n",
       "      <td>@AmericanAir on flight 1074 that has been dela...</td>\n",
       "      <td>negative</td>\n",
       "      <td>americanair flight 1074 delay hour human error...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12755</th>\n",
       "      <td>570036187815149570</td>\n",
       "      <td>@AmericanAir ok makes no sense tho Since you'l...</td>\n",
       "      <td>negative</td>\n",
       "      <td>americanair ok make sens tho sinc ll give free...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id                                               text  \\\n",
       "964    569988078015815680  @United will not have to honor absurdly low mi...   \n",
       "4175   567793696956231680  @united thanks for the offer, but I finally ma...   \n",
       "459    567846970141716481  @VirginAmerica @JezzieGoldz would have been a ...   \n",
       "12686  570062369096908801  @AmericanAir on flight 1074 that has been dela...   \n",
       "12755  570036187815149570  @AmericanAir ok makes no sense tho Since you'l...   \n",
       "\n",
       "      airline_sentiment                                       cleaned_text  \n",
       "964             neutral  unit honor absurd low mistak fare http vía usa...  \n",
       "4175           negative  unit thank offer final made destin albeit hour...  \n",
       "459            positive  virginamerica jezziegoldz would rough trip luc...  \n",
       "12686          negative  americanair flight 1074 delay hour human error...  \n",
       "12755          negative  americanair ok make sens tho sinc ll give free...  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and validation data\n",
    "X_train, X_val, y_train, y_val = train_test_split(df['cleaned_text'][1000:], \n",
    "                                                  df['airline_sentiment'][1000:], \n",
    "                                                  test_size=0.2, \n",
    "                                                  random_state=seed)\n",
    "\n",
    "# Get sentiment labels for test set\n",
    "y_test = test_set['airline_sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data vectorization\n",
    "\n",
    "Many machine learning models can only be trained on numerical input in the form of vectors or matrices. To prepare our tweets for the machine learning models we create a term frequency-inverse document frequency (tf-idf) vectorization. The result of this vectorization is a sparse matrix which contains a convenient representation of our tweets.\n",
    "\n",
    "The machine learning will learn which word frequency is important to predict a correct sentiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix based on word frequency in tweets\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_val = vectorizer.transform(X_val)\n",
    "X_test = vectorizer.transform(test_set['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 10912 tweets\n",
      "        Validation size: 2728 tweets\n",
      "        Test size: 1000 tweets\n",
      "        Amount of words (columns): 9643 words\n"
     ]
    }
   ],
   "source": [
    "# Print the size of our data\n",
    "print(f'Training size: {X_train.shape[0]} tweets\\n\\\n",
    "        Validation size: {X_val.shape[0]} tweets\\n\\\n",
    "        Test size: {X_test.shape[0]} tweets\\n\\\n",
    "        Amount of words (columns): {X_train.shape[1]} words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Models\n",
    "\n",
    "\n",
    "Multinomial Naive Bayes\n",
    "\n",
    "Multinomial Naive Bayes is the go-to method for text mining and is great to set a benchmark for sentiment analysis. It is easy to implement with sklearn's \"MultinomialNB\" class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multinomial Naive Bayes\n",
    "multi_nb = MultinomialNB()\n",
    "multi_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set (MultinomialNB): 73.3779%\n",
      "Accuracy on validation set (MultinomialNB): 66.4223%\n"
     ]
    }
   ],
   "source": [
    "# Check results\n",
    "train_pred = multi_nb.predict(X_train)\n",
    "val_pred = multi_nb.predict(X_val)\n",
    "print(f'Accuracy on training set (MultinomialNB): {round(accuracy_score(y_train, train_pred)*100, 4)}%')\n",
    "print(f'Accuracy on validation set (MultinomialNB): {round(accuracy_score(y_val,val_pred)*100, 4)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn's Gradient Boosting Machine (GBM)\n",
    "\n",
    "The Gradient Boosting Machine (GBM) is a powerful machine learning technique. The model basically trains decision trees models. It keeps training additive models on the residuals of the previous model. In this way we can get much better results than with a single decision tree. A GBM is however the most powerful if there is also plenty of labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=6,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=1234, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn's Gradient Boosting Classifier (GBM)\n",
    "gbm = GradientBoostingClassifier(n_estimators=200, \n",
    "                                 max_depth=6, \n",
    "                                 random_state=seed)\n",
    "gbm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=6,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=1234, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "              learning_rate=0.1, loss='deviance', max_depth=6,\n",
    "              max_features=None, max_leaf_nodes=None,\n",
    "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "              min_samples_leaf=1, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=200,\n",
    "              n_iter_no_change=None, presort='auto', random_state=1234,\n",
    "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
    "              verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set (GBM): 87.7199%\n",
      "Accuracy on validation set (GBM): 75.6965%\n"
     ]
    }
   ],
   "source": [
    "# Check results\n",
    "train_pred = gbm.predict(X_train)\n",
    "val_pred = gbm.predict(X_val)\n",
    "print(f'Accuracy on training set (GBM): {round(accuracy_score(y_train, train_pred)*100, 4)}%')\n",
    "print(f'Accuracy on validation set (GBM): {round(accuracy_score(y_val,val_pred)*100, 4)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## XGBoost (GBM)\n",
    "\n",
    "XGBoost is a Python package which provides an efficient implementation of a GBM. It often allows for more elaborate optimization than sklearn's GBM. In my experience, XGBoost often performs slightly better than sklearn's GBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:09:48] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters that you can tweak\n",
    "# There are a lot more tweakable hyperparameters that you can find at \n",
    "# https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "xgb_params = {'objective' : 'multi:softmax',\n",
    "              'eval_metric' : 'mlogloss',\n",
    "              'eta' : 0.1,\n",
    "              'max_depth' : 6,\n",
    "              'num_class' : 3,\n",
    "              'lambda' : 0.8,\n",
    "              'estimators' : 200,\n",
    "              'seed' : seed\n",
    "              \n",
    "}\n",
    "\n",
    "# Transform categories into numbers\n",
    "# negative = 0, neutral = 1 and positive = 2\n",
    "target_train = y_train.astype('category').cat.codes\n",
    "target_val = y_val.astype('category').cat.codes\n",
    "\n",
    "# Transform data into a matrix so that we can use XGBoost\n",
    "d_train = xgb.DMatrix(X_train, label = target_train)\n",
    "d_val = xgb.DMatrix(X_val, label = target_val)\n",
    "\n",
    "# Fit XGBoost\n",
    "watchlist = [(d_train, 'train'), (d_val, 'validation')]\n",
    "bst = xgb.train(xgb_params, \n",
    "                d_train, \n",
    "                400,  \n",
    "                watchlist,\n",
    "                early_stopping_rounds = 50, \n",
    "                verbose_eval = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set (XGBoost): 89.6811%\n",
      "Accuracy on validation set (XGBoost): 76.7595%\n"
     ]
    }
   ],
   "source": [
    "# Check results for XGBoost\n",
    "train_pred = bst.predict(d_train)\n",
    "val_pred = bst.predict(d_val)\n",
    "print(f'Accuracy on training set (XGBoost): {round(accuracy_score(target_train, train_pred)*100, 4)}%')\n",
    "print(f'Accuracy on validation set (XGBoost): {round(accuracy_score(target_val, val_pred)*100, 4)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Simple neural network\n",
    "\n",
    "Over the past years neural networks have given us extraordinary results on natural language processing (NLP) tasks with complex implementations. In this implementation however we will go back to basics and created a simple fully-connected neural networks with 20 neurons. The reason for this is that we have limited data and a small neural network model is already very prone to overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator so we can easily feed batches of data to the neural network\n",
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0]/batch_size\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The neural network requires us to create a one-hot encoding of our target variable (airline_sentiment). This means that we encode our classes in the following way:\n",
    "\n",
    "    negative = [1,0,0]\n",
    "    neutral = [0,1,0]\n",
    "    positive = [0,0,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Onehot encoding of target variable\n",
    "# Negative = [1,0,0], Neutral = [0,1,0], Positive = [0,0,1]\n",
    "\n",
    "# Initialize sklearn's one-hot encoder class\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# One hot encoding for training set\n",
    "integer_encoded_train = np.array(y_train).reshape(len(y_train), 1)\n",
    "onehot_encoded_train = onehot_encoder.fit_transform(integer_encoded_train)\n",
    "\n",
    "# One hot encoding for validation set\n",
    "integer_encoded_val = np.array(y_val).reshape(len(y_val), 1)\n",
    "onehot_encoded_val = onehot_encoder.fit_transform(integer_encoded_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.framework.ops' has no attribute '_TensorLike'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-153-ce6953fdb68b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Build model architecture\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\app\\anaconda3\\envs\\datascience_venv\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    164\u001b[0m                     \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m                     \u001b[1;31m# to the input layer we just created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m                     \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m                     \u001b[0mset_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\app\\anaconda3\\envs\\datascience_venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\app\\anaconda3\\envs\\datascience_venv\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    444\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\app\\anaconda3\\envs\\datascience_venv\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m                 \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m                 raise ValueError('Layer ' + self.name + ' was called with '\n",
      "\u001b[1;32md:\\app\\anaconda3\\envs\\datascience_venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mis_keras_tensor\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    693\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m     \"\"\"\n\u001b[1;32m--> 695\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    696\u001b[0m         raise ValueError('Unexpectedly found an instance of type `' +\n\u001b[0;32m    697\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\app\\anaconda3\\envs\\datascience_venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mis_tensor\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_TensorLike\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dense_tensor_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.python.framework.ops' has no attribute '_TensorLike'"
     ]
    }
   ],
   "source": [
    "# Neural network architecture\n",
    "initializer = keras.initializers.he_normal(seed=seed)\n",
    "activation = keras.activations.elu\n",
    "optimizer = keras.optimizers.Adam(lr=0.0002, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=4)\n",
    "\n",
    "# Build model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(20, activation=activation, kernel_initializer=initializer, input_dim=X_train.shape[1]))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax', kernel_initializer=initializer))\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "# Fit the model using the batch_generator\n",
    "hist = model.fit_generator(generator=batch_generator(X_train, onehot_encoded_train, batch_size=batch_size, shuffle=True),\n",
    "                           epochs=epochs, validation_data=(X_val, onehot_encoded_val),\n",
    "                           steps_per_epoch=X_train.shape[0]/batch_size, callbacks=[es])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "God is the Greatest for ever! I won a lottery. I'm so happy\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'df' and 'column_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-780cd6cf61b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"God is the Greatest for ever! I won a lottery. I'm so happy\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPreProcessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtokenized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'God'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'is'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'the'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Greatest'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'for'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ever'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'!'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'I'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'won'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lottery'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'I'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"'m\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'so'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'happy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPreProcessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove_stopwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'df' and 'column_name'"
     ]
    }
   ],
   "source": [
    "sentence = \"God is the Greatest for ever! I won a lottery. I'm so happy\"\n",
    "print(sentence)\n",
    "print(PreProcessing().tokenize(sentence))\n",
    "tokenized = ['God', 'is', 'the', 'Greatest', 'for', 'ever', '!', 'I', 'won', 'a', 'lottery', '.', 'I', \"'m\", 'so', 'happy']\n",
    "print(PreProcessing().remove_stopwords(tokenized))\n",
    "stopwords_removed = ['God', 'Greatest', 'ever', 'lottery', \"'m\", 'happy']\n",
    "print(PreProcessing().stem(stopwords_removed))\n",
    "stem_words = ['god', 'greatest', 'ever', 'lotteri', \"'m\", 'happi']\n",
    "print(PreProcessing().join_to_string(stem_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['God', 'is', 'the', 'Greatest', 'for', 'ever', '!', 'I', 'won', 'a', 'lottery', '.', 'I', \"'m\", 'so', 'happy']\n",
      "['God', 'Greatest', 'ever', 'lottery', \"'m\", 'happy']\n"
     ]
    }
   ],
   "source": [
    "# Removing stop words\n",
    "filtered_sentence = []\n",
    "stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "tokenized = ['God', 'is', 'the', 'Greatest', 'for', 'ever', '!', 'I', 'won', 'a', 'lottery', '.', 'I', \"'m\", 'so', 'happy']\n",
    "print(tokenized)\n",
    "for w in tokenized:\n",
    "    if w not in stop_words and len(w) > 1 and w[:2] != '//':\n",
    "        filtered_sentence.append(w)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
